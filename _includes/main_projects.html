<div style="display:flex;flex-flow:row wrap;">
<div style="flex: 1 1 0">
<span class="image left" style="max-width:100%"> 
  <a href="/projects/line-simp">
<img src="/assets/teaser-line-simp.png" alt=""/>
  </a>
</span>
</div>
<div style="flex: 3 1 0">
  <a href="/projects/line-simp"><h2 style="margin-top:0.1em">Region-Aware Simplification and Stylization of 3D Line Drawings</h2></a>
<p><strong>Vivien Nguyen</strong>, Matthew Fisher, Aaron Hertzmann, Szymon Rusinkiewicz</p>
<p>
Shape-conveying line drawings generated from 3D models normally create closed regions in image space. These lines and regions can be stylized to mimic various artistic styles, but for complex objects, the extracted topology is unnecessarily dense, leading to unappealing and unnatural results under stylization. Prior works typically simplify line drawings without considering the regions between them, and lines and regions are stylized separately, then composited together, resulting in unintended inconsistencies. We present a method for joint simplification of lines and regions simultaneously that penalizes large changes to region structure, while keeping regions closed. This feature enables region stylization that remains consistent with the outline curves and underlying 3D geometry.
</p>
</div>
</div>

<div style="display:flex;flex-flow:row wrap;">
<div style="flex: 1 1 0">
<span class="image left" style="max-width:100%"> 
<img src="/assets/teaser-imet.png" alt=""/>
</span>
</div>
<div style="flex: 3 1 0">
<h2 style="margin-top:0.1em">Cleaning and Structuring the Label Space of the iMet Collection 2020</h2>
<p><strong>Vivien Nguyen</strong>, Sunnie S.Y. Kim <em>(Equal Contribution)</em></p>
<p>
The iMet 2020 dataset is a valuable resource in the space of fine-grained art attribution recognition, but we believe it has yet to reach its true potential. We document the unique properties of the dataset and observe that many of the attribute labels are noisy, more than is implied by the dataset description. Oftentimes, there are also semantic relationships between the labels (e.g., identical, mutual exclusion, subsumption, overlap with uncertainty) which we believe are underutilized. We propose an approach to cleaning and structuring the iMet 2020 labels, and discuss the implications and value of doing so. Further, we demonstrate the benefits of our proposed approach through several experiments. Our code and cleaned labels are available at <a href="https://github.com/sunniesuhyoung/iMet2020cleaned" target="blank">this https URL</a>.
</p>
</div>
</div>

<div style="display:flex;flex-flow:row wrap;">
<div style="flex: 1 1 0">
<span class="image left" style="max-width:100%"> 
<a href="https://ceciliavision.github.io/vid-auto-focus/" target="blank">
<img src="/assets/teaser-syn-dof.jpg" alt=""/>
</a>
</span>
</div>
<div style="flex: 3 1 0">
<a href="https://ceciliavision.github.io/vid-auto-focus/" target="blank">
<h2 style="margin-top:0.1em">Synthetic Defocus and Look-Ahead Autofocus for Casual Videography</h2>
</a>
<p>Xuaner Zhang, Kevin Matzen, <strong>Vivien Nguyen</strong>, Dillon Yao, You Zhang, Ren Ng</p>
<p>
In cinema, large camera lenses create beautiful shallow depth of field (DOF), but make focusing difficult and expensive. Accurate cinema focus usually relies on a script and a person to control focus in realtime. Casual videographers often crave cinematic focus, but fail to achieve it. We either sacrifice shallow DOF, as in smartphone videos; or we struggle to deliver accurate focus, as in videos from larger cameras. This paper is about a new approach in the pursuit of cinematic focus for casual videography. We present a system that synthetically renders refocusable video from a deep DOF video shot with a smartphone, and analyzes future video frames to deliver context-aware autofocus for the current frame.
</p>
</div>
</div>
